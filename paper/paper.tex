%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb, amsmath}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\expd}{exp.}
%% The amsthm package provides extended theorem environments
\usepackage{amsthm}
\newtheorem{proposition}{Proposition}
\usepackage{hyperref}
\hyperbaseurl{}
\urlstyle{same}
\usepackage{caption}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\journal{Operations Research Letters}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
 \title{Dynamic estimation of multinomial logit choice models}
%% \tnotetext[label1]{}
 \author{Max Kapur}
 \ead{maxkapur@snu.ac.kr}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
 \affiliation{organization={Seoul National University}, city={Seoul}, country={South Korea}}
%% \fntext[label3]{}

\title{}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{}
%% \affiliation[label1]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%%
%% \affiliation[label2]{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}

\author{}

\affiliation{organization={},%Department and Organization
            addressline={}, 
            city={},
            postcode={}, 
            state={},
            country={}}

\begin{abstract}
Recent literature in choice model approximation has considered the dynamic data setting, in which new observations of consumer choices must be incorporated into the model but computational resources prevent the analyst from storing these observations in memory or solving the . Whereas research in this area has focused on nonparametric choice models, this paper considers the dynamic estimation of the multinomial logit choice (MNL) model and its nested variant. I provide a dynamic approximation scheme for the model parameters that updates the current estimate according to an intuitive rule and a sequence of decreasing step sizes. Stochastic convergence is guaranteed by the Robbins--Monro condition, and numerical results demonstrate the feasibility of the model.
\end{abstract}

%%Graphical abstract
\begin{graphicalabstract}
%\includegraphics{grabs}
\end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item Research highlight 1
\item Research highlight 2
\end{highlights}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% \linenumbers

%% main text


\section{Dynamic choice modeling}
In a recent paper, \cite{honguyen2021} articulate the need for dynamic choice-model fitting techniques that can efficiently update the current model in light of new observations of consumer behavior, rather than solving a full optimization problem over the perturbed data. However, that paper concerned nonparametric choice models, which are fundamentally hard to estimate even in a static setting. On the other hand, parametric choice offer a substantial advantage in computational tractability. To establish parity in the larger debate over the relative merits of parametric and nonparametric choice models, it is worth considering whether the tractability of parametric choice models also extends to the dynamic setting. 

The present paper takes a few first steps in this direction by providing a dynamic approximation scheme for the multinomial logit choice model and its nested variant. When a new observation of a consumer choice given a certain consideration set comes in, the technique proposed here updates the current parameter estimate according to a simple rule with a sequence of decreasing step sizes. Stochastic convergence is guaranteed by the Robbins-Monro condition. I demonstrate the feasibility of the technique through numerical results. 

\section{The nested multinomial logit choice model}
Consider a set of products $\mathcal{N} = \{1 \dots n\}$. A consumer is offered a choice among an assortment of products $\mathcal{A} \subseteq \mathcal{N}$. A choice model is a probabilistic model that defines the probability that the consumer will choose product $i$.

In the multinomial logit (MNL) choice model, each consumer $j$'s preference order over the products in $\mathcal{A}$ is determined by the sum of a systematic preferability parameter $\delta_i$ and a noise term $\epsilon_{ij}$. The noise terms have an iid extreme value distribution, and the consumer chooses the product in $\mathcal{A}$ for which $\delta_i + \epsilon_{ij}$ is greatest. It can be shown that the probability of a given consumer choosing product $i$ from the assortment $\mathcal{A}$ is
\begin{equation}\label{mnlchoiceprobability}
\operatorname{Pr}\left[\;\text{choose } i ~|~\text{assortment }\mathcal{A}\;\right] =\frac{\exp \delta_i}{\sum_{h\in \mathcal{A}} \exp \delta_h}
\end{equation}
The value of this equation does not change when each $\delta_i$ is perturbed by the same constant; hence, we may assume without loss of generality that $\sum_i \delta_i = 0$.

A known issue with the MNL choice model is the assumption that introducing a new item into the assortment does not alter the preferability of the items already in the assortment relative to one another. This assumption, known as the independence of irrelevant alternatives, is violated when some products are near substitutes. The \emph{nested} multinomial logit model (NMNL) addresses this issue by grouping similar products into nests, notated $k \in \{ 1 \dots s\}$. Under NMNL, the customer first chooses a nest $k$, then a product $i$. Both choices are made using MNL. Each nest's preferability parameter is written $\sigma_k$, while the products within the nest have preferability parameters $\delta_{ki}$. For convenience, assume each nest contains $n$ products. Hence $\mathcal{N} = \{1\dots s\} \times \{1\dots n\}$.

We can represent a given assortment $\mathcal{A}$ as a list of (nest, product) tuples $(k, i)$. The assortment may span the $s$ nests and $sn$ products in any arbitrary way. Let $\mathcal{A}^{\text{nests}}$ denote the set of nests represented in the assortment $\mathcal{A}$, and let $\mathcal{A}_k^{\text{products}}$ denote the set of products that are in both nest $k$ and the assortment $\mathcal{A}$. Then the probability that a given customer chooses the product $(k, i)$ is
\begin{equation}\label{nmnlchoiceprobability}
\operatorname{Pr}\left[\;\text{choose } (k, i) ~|~\text{assortment }\mathcal{A}\;\right] =\frac{\exp \sigma_k}{\sum_{l\in \mathcal{A}^{\text{nests}}} \exp \sigma_l} \cdot \frac{\exp \delta_i}{\sum_{h\in\mathcal{A}_{k}^{\text{products}}} \exp \delta_h}\end{equation}
where we may assume without loss of generality that $\sum_k \sigma_k = 0$ and $\sum_i \delta_{ki} = 0, \forall k$. 

\section{Static parameter estimation}
Consider the static parameter estimation task in the MNL choice model. $m$ consumers, indexed by $j$, were shown assortments $\mathcal{A}_j \subseteq \mathcal{N}$ and asked to pick their favorite product $y_j$. For convenience, abbreviate the preferability parameter associated with consumer $j$'s choice from $\delta_{y_j}$ to $\delta_j$. 

The likelihood of the $m$ observations with respect to the parameters $\delta$ is
\begin{equation}
L(\delta) = \prod_{j=1}^m \frac{\exp \delta_j}{\sum_{h\in \mathcal{A}_j} \exp \delta_h}
\end{equation}
which is log-concave in $\delta$. 

Now consider the NMNL choice model. Each consumer $j$ chooses a product from an assortment $\mathcal{A}_j$, and we may write the consumer's choice as $(x_j, y_j)$, where $y_j$ is the product chosen and $x_j$ is the nest to which $y_j$ belongs. (Note that this requires that the nests be designated in advance.) Again, $\mathcal{A}_j$ as a list of (nest, product) tuples $(k, i)$. Let $\mathcal{A}_j^{\text{nests}}$ denote the set of nests represented in the assortment $\mathcal{A}_j$, let $\mathcal{A}_{j}^{\text{products}}$ denote the set of products that are in both $x_j$ and the assortment $\mathcal{A}_j$, and abbreviate the parameters $(\sigma_{x_j}, \delta_{y_j})$ to $(\sigma_j, \delta_j)$. Then the likelihood function is
\begin{equation}L(\sigma, \delta) = \prod_{j=1}^m \frac{\exp \sigma_j}{\sum_{l\in \mathcal{A}_j^{\text{nests}}} \exp \sigma_l} \cdot \frac{\exp \delta_j}{\sum_{h \in \mathcal{A}_{j}^{\text{products}}} \exp \delta_h}\end{equation}

Computational tools for static estimation of variants of the MNL choice model are widely available \cite[][]{croissantnd}. 

\section{Parameter estimation in the data stream framework}
To frame the discussion of dynamic parameter estimation, I consider an online setting called the data stream. In the data stream, we are only allowed to store our \emph{current} parameter estimate $\delta$ and a state variable $t$ representing the number of observations used to form the current estimate. When a new observation $(\mathcal{A}, y)$ comes in, we can update the stored variables, but we cannot store the observation itself. %A simple example of a parameter estimation scheme in the data stream framework is the updating sample mean $\bar x^{(t+1)} = \frac{1}{t+1} \left( x_{t+1} + t \bar x^{(t)}\right)$. This calculation eliminates the need to store past observations of the random variable $X$, and each update is $\mathcal{O}(1)$ instead of the $\mathcal{O}(t)$ operations required to recalculate the mean over the full dataset. The MNL parameter update described below offers an analogous computational advantage.

\subsection{The MNL parameter update}
First, consider the estimation of MNL choice parameters. At time $t$, the parameter estimate is $\delta^{(t)}$. A new observation arrives in which a consumer with consideration set $\mathcal{A}_t$ chose product $y_t$. 

Let $\delta_{\mathcal{A}}$ denote the vector of preferability parameters corresponding to the products in $\mathcal{A}$. Let $e_{y}$ denote the vector of the same length with a one at the index corresponding to $y$ and zeros elsewhere. Finally, let
\begin{equation}z^{(t)} = e_{y_t} -  \frac{1}{\sum_{i\in \mathcal{A}_t} \exp \delta_i} \expd \delta^{(t)}_{\mathcal{A}_t}\end{equation}
where $\expd x = (\exp x_1, \dots, \exp x_n)$.

I propose the following \emph{MNL parameter update}:
\begin{align} \label{mnlparameterupdate}
\delta^{(t+1)}_{\mathcal{A}_t} = \delta^{(t)}_{\mathcal{A}_t} + \frac{\alpha}{t^r} z^{(t)} \qquad\qquad
\delta^{(t+1)}_{\mathcal{N} \setminus \mathcal{A}_t} = \delta^{(t)}_{\mathcal{N} \setminus \mathcal{A}_t}
\end{align}
where $\alpha >0$ and $r \in (0, 1]$ are step parameters.

\begin{proposition}If the observations are drawn from an MNL choice model and each consideration set appears with nonzero probability, then the MNL parameter update converges in expectation to the true parameters $\delta$.
\end{proposition}

The intuition behind this parameter update is as follows. Seeing that $y_t$ was chosen from $\mathcal{A}_t$, we 
\begin{itemize}
\item Increase the estimated preferability of the item that was picked,
\item Decrease the estimated preferability of the items that were rejected, and
\item Leave unchanged the preferability of items not in the presented assortment.
\end{itemize}
Notice that $\sum_{i=1}^n \delta_i$ remains constant under the parameter update, because $\sum_i z_i = 0$. 

\subsection{A brief example}
Suppose the parameter estimate is $\delta^{(1)} = (-2, 1, 3, -2)$ when we observe a consumer choose item $y_1 = 4$ from the consideration set  $\mathcal{A}_1 = \{2, 4\}$. Then when $\alpha = 1$, 
\begin{align}
\delta_{\mathcal{A}_1}^{(1)} =  \begin{bmatrix}1\\ -2\end{bmatrix}, \qquad e_{y_1} = \begin{bmatrix}0\\1\end{bmatrix} 
\end{align}
and
\begin{align}
\delta_{\mathcal{A}_2}^{(2)} &= \begin{bmatrix}1\\ -2\end{bmatrix} +
\frac{1}{1}\left(\begin{bmatrix}0\\ 1\end{bmatrix} - \frac{1}{e^1 + e^{-2}}\begin{bmatrix}e^1\\ e^{-2}\end{bmatrix}\right) =
\begin{bmatrix}0.05 \\ -1.05 \end{bmatrix} \\
\implies \quad \delta^{(2)} &= (-2, 0.05, 3, -1.05)
\end{align}
is the updated parameter estimate.

\subsection{Why the parameter update converges}
Consider the multivariate root-finding problem \[\text{find } \theta^*: F(\theta^*) = 0\]
where the Jacobian of $F: \mathbb{R}^n \mapsto \mathbb{R}^n$ is negative definite (ND), and the iterative scheme \[\theta^{(t+1)} = \theta^{(t)} + \frac{\alpha}{t^r} F\left(\theta^{(t)} \right)\]
where $\alpha >0$ and $r \in (0, 1]$ are step parameters. Such an iterative scheme is known to converge to the root in a variety of scenarios %always?
; a typical example is maximizing a concave function whose gradient is $F$ using gradient ascent. However, \cite{robbinsmonro1951} showed (in the univariate case) that one can replace $F(\theta)$ with an \emph{experiment} $\hat F(\theta)$ satisfying $\mathbb{E}\left[\hat F(\theta)\right] = F(\theta)$. The same iterative scheme exhibits \emph{stochastic convergence}, meaning that with probability one,
\[\lim_{t\to \inf} \mathbb{E}\left[F\bigl(\theta^{(t)}\bigr)\right] = \theta^*\]
Many convergence rate results also generalize to stochastic interpretations.

A classic example of a Robbins--Monro algorithm is stochastic gradient descent, which replaces the sum of error terms in a loss function with the errors of a randomly chosen subset of samples. In fact, the MNL parameter update can be regarded as a form of stochastic gradient descent for the log likelihood function, because when the single observation $(\mathcal{A}_t, y_t)$ constitutes the full dataset, the maximum likelihood estimator for the preferability of the products in $\mathcal{A}_t$ is precisely $e_{y_t}$. I offer a somewhat more targeted argument, however, in the following subsection.

\subsection{The MNL parameter update is a Robbins--Monro algorithm}
% Throughout here, incorporate ``given that A is the assortment''
In the MNL parameter estimation task, it is easy to see that for a given assortment $\mathcal{A}$, the expected value of the one-hot vector $e_y$ is the probability weight of each item in the assortment:
\begin{equation}\mathbb{E}[e_{y}]  = \frac{1}{\sum_{i\in \mathcal{A}} \exp \delta_i} \expd \delta_{\mathcal{A}}\end{equation}
The counterpart to $F$ is the expected value of the random function $z$.
\begin{equation}\mu(z) = \mathbb{E}[z] = \mathbb{E}\left[e_{y} -  \frac{1}{\sum_{i\in \mathcal{A}} \exp \delta_i} \expd \delta_{\mathcal{A}} \right]\end{equation}
Assuming that each consideration set $\mathcal{A} \in \mathcal{N}!$ is presented with nonzero probability, $\mu(z) = 0$ if and only if $\delta$ is correct. Observe that the Jacobian of $\mu(z)$ in $\delta$ is ND. 

The counterpart to the experiment $\hat F$ is the realization $z^{(t)}$, which is alike in expected value.

%To complete the argument, it is necessary to generalize the expectation of E[ey] over all the possible assortments. This can be done by redefining $z$ and $e$ so that they produce the indices for all products. Then the expectation can be computed by summing over A probabilities in the normal way.

\subsection{The NMNL parameter update}
The NMNL parameter update is analogous to the MNL parameter update. Each observation may be written as a tuple $(\mathcal{A}_t, x_t, y_t)$. Define $\mathcal{A}_t^{\text{nests}}$ and $\mathcal{A}_{t}^{\text{products}}$ as above, and let $e_x$ and $e_y$ denote the vectors having the same lengths as $\mathcal{A}_t^{\text{nests}}$ and $\mathcal{A}_{t}^{\text{products}}$, where $e_x$ has a one at the index corresponding to $x$, $e_y$ has a one at the index corresponding to $y$, and the other entries are zero. Finally, let
\begin{align}
w^{(t)} &= e_{x_t} -  \frac{1}{\sum_{i\in \mathcal{A}_{t}^{\text{nests}}} \exp \sigma_i} \expd \sigma^{(t)}_{\mathcal{A}_{t}^{\text{nests}}} \\
z^{(t)} &= e_{y_t} -  \frac{1}{\sum_{i\in \mathcal{A}_{t}^{\text{products}}} \exp \delta_i} \expd \delta^{(t)}_{\mathcal{A}_{t}^{\text{products}}}
\end{align}
The parameters are updated as follows.
\begin{align}\label{nmnlparameterupdate}
\sigma^{(t+1)}_{\mathcal{A}_t^{\text{nests}}} = \sigma^{(t)}_{\mathcal{A}_t^{\text{nests}}} + \frac{\beta}{t^r} w^{(t)} \qquad\qquad
\sigma^{(t+1)}_{\mathcal{N} \setminus \mathcal{A}_t^{\text{nests}}} = \sigma^{(t)}_{\mathcal{N} \setminus \mathcal{A}_t^{\text{nests}}} \\
\delta^{(t+1)}_{\mathcal{A}_t^{\text{products}}} = \delta^{(t)}_{\mathcal{A}_t^{\text{products}}} + \frac{\alpha}{t^r} z^{(t)} \qquad\qquad
\delta^{(t+1)}_{\mathcal{N} \setminus \mathcal{A}_t^{\text{products}}} = \delta^{(t)}_{\mathcal{N} \setminus \mathcal{A}_t^{\text{products}}}
\end{align}
where $\beta, \alpha >0$ and $r \in (0, 1]$ are step parameters.

\begin{proposition}If the observations are drawn from an NMNL choice model and each consideration set appears with nonzero probability, then the NMNL parameter update converges to the true parameters $\sigma$ and $\delta$.\end{proposition}

\section{Computational examples}
I demonstrate the tractability of the dynamic parameter update through three simulations. The first simulation, whose results are shown in Figure \ref{param-est-dynamic-nonnested}, tests the MNL parameter update, while the second simulation considers the NMNL case. In both simulations, a small number of products was chosen to yield a legible graph; the parameter update formula is computationally tractable for large numbers of products. The third simulation compares maximum likelihood estimation over a full dataset with the average of the final 1000 iterations of a dynamic estimation of the same. While the accuracy is necessarily lower in the latter case, it is acceptable given the savings in computation time. In all cases, it is possible to tune the step parameters $\alpha$, $\beta$, and $r$ to obtain smoother, slower convergence. The Julia code for all three simulations is available on Github at \url{https://github.com/maxkapur/MultinomialLogit}. 

\begin{figure}
\begin{center}\includegraphics[width=\textwidth]{../plots/param-est-dynamic-nonnested.pdf}\end{center}
\captionsetup{singlelinecheck=off}
    \caption[.]{Dynamic estimation of MNL choice parameters for a collection of $n = 7$ products. True preferability parameters $\delta$ and the initial estimate $\delta^{(1)}$ were drawn independently from a normal distribution and centered at zero. At times $t = 1 \dots 25000$, a random assortment consisting of between two and five products was drawn, a consumer chose a product using the MNL choice model, and the parameter estimates were updated according to the MNL parameter update (Equation \ref{mnlparameterupdate}). The parameter estimate at each iteration is shown in a light stroke, while the darker lines show a moving average over the previous 500 estimates. The step parameters are $\alpha = 0.01$ and $r = 0.05$.}
\label{param-est-dynamic-nonnested}
\end{figure}


\begin{figure}
\begin{center}\includegraphics[width=\linewidth, ]{../plots/param-est-dynamic-nested.pdf}\end{center}
\captionsetup{singlelinecheck=off}
    \caption[.]{Dynamic estimation of MNL choice parameters for a collection of $sn = 12$ products divided across $s=4$ nests. True preferability parameters $\sigma$ and $\delta$ were drawn independently from a normal distribution and centered at zero, while the initial parameter estimates were set to zero. At times $t = 1 \dots 60000$, a random assortment consisting of between two and five products was drawn, a consumer chose a product using the MNL choice model, and the parameter estimates were updated according to the NMNL parameter update (Equation \ref{nmnlparameterupdate}). The parameter estimate at each iteration is shown in a light stroke, while the darker lines show a moving average over the previous 1200 estimates. The step parameters are $\beta = 0.01$, $\alpha = 0.03$, and $r = 0.05$. The first pane shows the time series of $\sigma$ estimates, while the second shows the entries of $\delta$, color-coded by nest membership.}
\label{param-est-dynamic-nested}
\end{figure}



\begin{figure}
\begin{center}\includegraphics[width=\linewidth, ]{../plots/param-est-dynamic-nested-as-static.pdf}\end{center}
\captionsetup{singlelinecheck=off}
    \caption[.]{Performance of the dynamic parameter estimate versus maximum likelihood estimation. Preferability parameters for $sn = 120$ products divided into $s=10$ nests were drawn from the normal distribution and centered at zero. $m=50000$ assortments and consumer choices were generated according to the NMNL model. In the upper two panes, the observations were treated as a time series, and I report the average of the final 1000 estimates produced by the parameter update with step parameters $\beta = 0.01$, $\alpha = 0.03$, and $r = 0.05$. In the lower two panes, maximum likelihood estimation was used to estimate the parameters. Dynamic parameter estimation offers acceptable accuracy at substantially lower computational cost.}
\label{param-est-dynamic-nested-as-static}
\end{figure}





\section{Conclusion}
Recent research in choice models has undergone two shifts in emphasis: one toward nonparametric choice models, and another toward the estimation thereof in a dynamic data setting. This paper has attempted to decouple these  by considered the dynamic estimation of \emph{parametric} choice models in the MNL family. Preliminary results suggest that the computational advantage of parametric choice models is also evident in the dynamic data setting. Whereas the algorithms suggested by \cite{honguyen2021} require solving a combinatorial optimization subproblem at each iteration, the complexity of parameter update step proposed here is linear in the number of items and does not require subiterative computation. Of course, all parametric choice models impose assumptions on the underlying distribution of customer preference orders that may not be appropriate in all settings, and the larger debate about the relative merits of parametric and nonparametric models is far from settled.

A natural extension of the ideas proposed here is parameter estimation in other choice models within and beyond the MNL family. 

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

%% If you have bibdatabase file and want bibtex to generate the
%% bibitems, please use
%%
%%  \bibliographystyle{elsarticle-harv} 
%%  \bibliography{<your bibdatabase>}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

\begin{thebibliography}{00}

%% \bibitem[Author(year)]{label}
%% Text of bibliographic item
\bibitem[Croissant(n.d.)]{croissantnd}
Croissant, Yves. ``Estimation of multinomial logit models in R: The \texttt{mlogit} Packages." {\url{https://www-eio.upc.es/teaching/madt/apunts/mlogit_teoria.pdf}}. 

\bibitem[Ho-Nguyen and Kılınç-Karzan(2021)]{honguyen2021}
Ho-Nguyen, Nam and Fatma Kılınç-Karzan. 2021. ``Dynamic Data-Driven Estimation of Nonparametric Choice Models.'' \emph{Operations Research: Articles in Advance.} {\url{https://doi.org/10.1287/opre.2020.2077}}.

\bibitem[Robbins and Monro(1951)]{robbinsmonro1951}
Robbins, Herbert and Sutton Monro. 1951. ``A Stochastic Approximation Method." \emph{The Annals of Mathematical Statistics} 22(3): 400-407. {\url{https://doi.org/10.1214/aoms/1177729586}}. 


\end{thebibliography}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-harv.tex'.
